This Repo is based on [SimCSE](https://github.com/princeton-nlp/SimCSE), specifically on its unsupervised learning framework for Contrastive Learning.

To reveal the magic behind BERT/RoBERTa as the extractor of embeddings, we replace the embedding extractor from BERT/roBERTa to albert, gemma and llama2. It's basically a comparison about bi-directional and uni-directional attension mechanism.